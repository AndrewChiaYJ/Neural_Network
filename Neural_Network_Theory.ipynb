{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cb9dd1-5fd6-4abb-8dda-e6265b6ceb58",
   "metadata": {},
   "source": [
    "# Neural Network - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d9171-e3f0-4379-a864-65beb81bfc86",
   "metadata": {},
   "source": [
    "# What is a Neural Network?\n",
    "\n",
    "Neural Network forms the base of deep learning, which is a subset of the machine learning field.\n",
    "- It is inspired by the structure of human brain.\n",
    "- A real neuron in human brain has the following components:\n",
    "  - Dendrite: Input to a Neuron\n",
    "  - Cell body: Information processing happens here\n",
    "  - Axon: Output to the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720db51d-534d-4288-9416-c65017135e54",
   "metadata": {},
   "source": [
    "# Components of Artificial Neural Network\n",
    "\n",
    "1. Input layer: Receives input as an array of data\n",
    "2. Output layer: Predicts final output\n",
    "3. Hidden layers: A black box which perform most computations required\n",
    "4. Neurons: Each node is represented as neurons, similar to neurons of brain.\n",
    "5. Channels: The connections connecting 2 neurons.\n",
    "6. Weights: Each channels is assigned a numerical value, for which the input from one neuron would be multipled with this weight and supply to the connecting neuron.\n",
    "7. Bias: Neurons in the hidden layers which received inputs from input layer would be associated with a numerical value. This numerical value is called bias.\n",
    "8. Activation Function: A mathematical function that would determine whether the threshold has been crossed for a neuron to be activated & translates the data to the next neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f595b-a1d6-4922-991a-9489e738c71e",
   "metadata": {},
   "source": [
    "# How Artificial Neural Network work?\n",
    "1. The input is being feed as arrays of data into the input layer.\n",
    "2. Random weights are assigned to each interconnection between the input and hidden layer.\n",
    "3. The weights are multipled with the inputs and a bias is added to form the transfer function.\n",
    "$$z = \\sum_{i=1}^n w_i x_i + b$$\n",
    "\t•\t z : The result of the summation (input to the activation function).\\\n",
    "\t•\t w_i : Weight associated with each input  x_i.\\\n",
    "\t•\t x_i : Individual inputs to the node.\\\n",
    "\t•\t b : Bias term added to the summation.\\\n",
    "\t•\t $\\sum_{i=1}^n$ : Summation symbol, summing over  i  from 1 to  n  (total number of inputs).\n",
    "\n",
    "4. Weights are assigned to the interconnection between the hidden layers.\n",
    "5. The output of a transfer function (from 1 hidden layer) is fed as an input to the activation function (of the subsequent hidden layer).\n",
    "6. At the end, the output layer would output the final form of a prediction, by applying suitable activation function to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533f9d9-6221-4aab-8b3b-0994eb473350",
   "metadata": {},
   "source": [
    "# Weights\n",
    "The higher a weight of an artificial neuron is, the stronger the input which is\n",
    "multiplied by it will be. Weights can also be negative, so we can say that the signal is\n",
    "inhibited by the negative weight.\n",
    "\n",
    "Depending on the weights, the computation of the neuron will be different. By adjusting the weights of an artificial neuron we can obtain the output we want for specific inputs. \n",
    "\n",
    "But when we have an ANN of hundreds or thousands of neurons, it would be quite complicated to find by hand all the necessary weights. But we can find algorithms which can adjust the weights of the ANN in order to obtain the desired output from the network. This process of adjusting the weights is called learning or training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de713c5-0aaf-4adf-aa83-aa3084c6afa0",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "There are multiple types of activation function that could be used.\n",
    "\n",
    "1. the Sigmoid Function, $sigma(z) = \\frac{1}{1 + e^{-z}}$, which is used when the model is predicting probability.\n",
    "<img src=\"sigmoid_function_plot.png\" alt=\"Sigmoid Function\" width=\"500\">\n",
    "\n",
    "2.  the Threshold Function, $\n",
    "\\phi(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$ , which is used when the output depends on a threshold value.\n",
    "<img src=\"threshold_function_plot.png\" alt=\"Threshold Function\" width=\"500\">\n",
    "\n",
    "4. the ReLU(Rectified Linear Unit) Function, which gives an output x if x is positive, 0 otherwise.\n",
    "\n",
    "$$\\phi(x) = \\max(0, x)$$\n",
    "\n",
    "Where:\n",
    "- \\(x\\): Input value.\n",
    "- $(\\phi(x))$: Output of the ReLU function.\n",
    "<img src=\"relu_function_plot.png\" alt=\"ReLU Function\" width=\"500\">\n",
    "\n",
    "5. the Hyperbolic Tangent Function - similar to sigmoid function with a range of (-1,1).\n",
    "\n",
    "\n",
    "$$\\phi(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "Where:\n",
    "- $(x)$: Input value.\n",
    "- $(\\phi(x))$: Output of the tanh function.\n",
    "<img src=\"tanh_function_plot.png\" alt=\"Tanh Function\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f5369-43da-4e18-8345-cce832eb0066",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the process of updating the weights of the network in order to reduce the error in prediction.\\\n",
    "The backpropagation algorithm uses supervised learning, which means that we provide the algorithm with examples of the inputs and outputs we want the network to compute, and a cost function is calculated (taking into account the magnitude of loss at any point on our graph, combined with the slope).\\\n",
    "The output is compared with the original result and multiple iterations are done to get the maximum accuracy. \\\n",
    "For practical reasons, ANNs implementing the backpropagation algorithm do not\n",
    "have too many layers, since the time for training the networks grows exponentially. Also, there are refinements to the backpropagation algorithm which allow a faster learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35daa6-9861-4cf0-ad82-eb2991f9cd4c",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "It is a measurement of error which defines the precision lost on comparing the predicted output to the actual output.\n",
    "\n",
    "Loss Function Formula:[(actual output) - (predicted output)]<sup>2</sup>\n",
    "\n",
    "## Error of the entire network\n",
    "The error of the network will simply be the sum of the errors of all the neurons in the output layer:\n",
    "$\\sum_{i=1}^n$[(actual output) - (predicted output)]<sup>2</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe28bf-5be5-4440-a631-989a288275cc",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "A graphical method of finding the minimum of a function.\\\n",
    "A random point on this curve is chosen and the slope at this point is calculated.\n",
    "- A +ve slope == an increase in weight\n",
    "- A -ve slope == a decrease in weight\n",
    "- A zero slope == appropriate weight\n",
    "  \n",
    "Our aim is to reach a point where the slope is zero.\n",
    "\n",
    "The formula for gradient descent is: $$\\Delta w_{ji} = - \\eta \\frac{\\partial E}{\\partial w_{ji}}$$\\\n",
    "\t•\t$\\Delta w_{ji}$: The change in weight for the connection from neuron $j$ to neuron $i$.\\\n",
    "\t•\t$\\eta$: The learning rate.\\\n",
    "\t•\t$\\frac{\\partial E}{\\partial w_{ji}}$: The partial derivative of the error $E$ with respect to the weight $w_{ji}$, which represents the gradient.\n",
    "\n",
    "This formula can be interpreted in the following way: the adjustment of each weight\n",
    "${\\Delta w_{ji}}$ will be the negative of a constant $\\eta$ multiplied by the dependance of the\n",
    "i previous weight on the error of the network, which is the derivative of E in respect to ${w_{i}}$.\n",
    "The size of the adjustment will depend on $\\eta$, and on the contribution of the weight to the error of the function. This is, if the weight contributes a lot to the error, the  adjustment will be greater than if it contributes in a smaller amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f90eba-063d-4ea7-9fc6-1108f43dfc55",
   "metadata": {},
   "source": [
    "# Types of Neural Network\n",
    "\n",
    "### 1. Feedforward Neural Network\n",
    "Simplest form of Artificial Neural Network, data travels only in 1 direction (input -> output)\n",
    "- Applications: Vision and speech recognition\n",
    "\n",
    "### 2. Radial Basis Function Neural Network\n",
    "This model classifies the data point based on its distance from a center point.\n",
    "- Applications: Power Restoration Systems\n",
    "  \n",
    "### 3. Kohonen Self Organizing Neural Network\n",
    "Vectors of random dimensions are input to discrete map comprised of neurons.\n",
    "- Applications: Used to recognizer patterns in data like in medical analysis.\n",
    "\n",
    "### 4. Recurrent Neural Network (RNN)\n",
    "The hidden layer saves its output to be used for future prediction\n",
    "- Applications: Text to speech conversion model\n",
    "\n",
    "### 5. Convolution Neural Network (CNN)\n",
    "The input features are taken in batches like a filter. This allows the network to remember an image in parts!\n",
    "- Applications: Used in signal and image processing\n",
    "\n",
    "### 6. Modular Neural Network\n",
    "It has a collection of different neural networks working together to get the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c45dd5-b21c-485d-af15-50b3bc62d4e0",
   "metadata": {},
   "source": [
    "# Applications of neural networks\n",
    "- Facial Recognition\n",
    "- Handwriting Recognition\n",
    "- Forecasting\n",
    "    - Stock exchange prediction\n",
    "- Music Composition\n",
    "- Image Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445dc0f-1b22-48e8-bd3a-e5fdc08009a4",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314aa95-daaa-4c51-9977-2b2d0a5e5210",
   "metadata": {},
   "source": [
    "1. [Artificial Neural Networks for Beginners by Carlos Gershenson](https://arxiv.org/abs/cs/0308031)\n",
    "2. [Neural Network Full Course | Neural Network Tutorial For Beginners | Neural Networks | Simplilearn](https://www.youtube.com/watch?v=ob1yS9g-Zcs&ab_channel=Simplilearn)\n",
    "3. [Neural Networks in Python – A Complete Reference for Beginners](https://www.askpython.com/python/examples/neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4edfe-31e5-491f-83f3-8bcb732299a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
