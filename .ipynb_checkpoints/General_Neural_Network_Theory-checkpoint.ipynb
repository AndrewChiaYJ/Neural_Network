{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cb9dd1-5fd6-4abb-8dda-e6265b6ceb58",
   "metadata": {},
   "source": [
    "# Neural Network - Theory\n",
    "\n",
    "## Contents:\n",
    "\n",
    "### [1. General Knowledge about Neural Network](#What-is-a-Neural-Network?)\n",
    "####    [- Types of Neural Network](#Types-of-Neural-Network)\n",
    "####    [- Applications of neural networks](#Applications-of-neural-networks)\n",
    "####    [- Components of Artificial Neural Network](#Components-of-Artificial-Neural-Network)\n",
    "####    [- How Artificial Neural Network work?](#How-Artificial-Neural-Network-work?)\n",
    "####    [--- Weights](#Weights)\n",
    "####    [--- Activation Function](#Activation-Function)\n",
    "####    [--- Backpropagation](#Backpropagation)\n",
    "####    [--- Loss Function](#Loss-Function)\n",
    "####    [--- Gradient Descent](#Gradient-Descent)\n",
    "####    [- How do we know the number of layers and their types?](#How-do-we-know-the-number-of-layers-and-their-types?)\n",
    "\n",
    "### [2. Restricted Boltzmann Machines (RBM)](#Restricted-Boltzmann-Machines-(RBM))\n",
    "####    [--- RBM - Scores](#RBM---Scores)\n",
    "####    [--- Formula of RBM Energy Function](#Formula-of-RBM-Energy-Function)\n",
    "####    [--- RBM - Probabilities](#RBM---Probabilities)\n",
    "####    [--- Contrastive Divergence](#Training---Contrastive-Divergence)\n",
    "####    [--- Gibbs Sampling](#Gibbs-Sampling)\n",
    "####    [- Difference between RBM & a normal Feed Forward Network](#Difference-between-RBM-&-a-normal-Feed-Forward-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d9171-e3f0-4379-a864-65beb81bfc86",
   "metadata": {},
   "source": [
    "# What is a Neural Network?\n",
    "\n",
    "Neural Network forms the base of deep learning, which is a subset of the machine learning field.\n",
    "- It is inspired by the structure of human brain.\n",
    "- A real neuron in human brain has the following components:\n",
    "  - Dendrite: Input to a Neuron\n",
    "  - Cell body: Information processing happens here\n",
    "  - Axon: Output to the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f90eba-063d-4ea7-9fc6-1108f43dfc55",
   "metadata": {},
   "source": [
    "# Types of Neural Network\n",
    "\n",
    "### 1. Feedforward Neural Network\n",
    "Simplest form of Artificial Neural Network, data travels only in 1 direction (input -> output)\n",
    "- Applications: Vision and speech recognition\n",
    "\n",
    "### 2. Radial Basis Function Neural Network\n",
    "This model classifies the data point based on its distance from a center point.\n",
    "- Applications: Power Restoration Systems\n",
    "  \n",
    "### 3. Kohonen Self Organizing Neural Network\n",
    "Vectors of random dimensions are input to discrete map comprised of neurons.\n",
    "- Applications: Used to recognizer patterns in data like in medical analysis.\n",
    "\n",
    "### 4. Recurrent Neural Network (RNN)\n",
    "The hidden layer saves its output to be used for future prediction\n",
    "- Applications: Text to speech conversion model\n",
    "\n",
    "### 5. Convolution Neural Network (CNN)\n",
    "The input features are taken in batches like a filter. This allows the network to remember an image in parts!\n",
    "- Applications: Used in signal and image processing\n",
    "\n",
    "### 6. Modular Neural Network\n",
    "It has a collection of different neural networks working together to get the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c45dd5-b21c-485d-af15-50b3bc62d4e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Applications of neural networks\n",
    "- Facial Recognition\n",
    "- Handwriting Recognition\n",
    "- Forecasting\n",
    "    - Stock exchange prediction\n",
    "- Music Composition\n",
    "- Image Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720db51d-534d-4288-9416-c65017135e54",
   "metadata": {},
   "source": [
    "# Components of Artificial Neural Network\n",
    "\n",
    "1. Input layer: Receives input as an array of data\n",
    "2. Output layer: Predicts final output\n",
    "3. Hidden layers: A black box which perform most computations required\n",
    "4. Neurons: Each node is represented as neurons, similar to neurons of brain.\n",
    "5. Channels: The connections connecting 2 neurons.\n",
    "6. Weights: Each channels is assigned a numerical value, for which the input from one neuron would be multipled with this weight and supply to the connecting neuron.\n",
    "7. Bias: Neurons in the hidden layers which received inputs from input layer would be associated with a numerical value. This numerical value is called bias.\n",
    "8. Activation Function: A mathematical function that would determine whether the threshold has been crossed for a neuron to be activated & translates the data to the next neurons.\n",
    "\n",
    "## How to count layers?\n",
    "There are disagreements on whether to count the input layer as a layer, but generally the convention is that they should not be counted (supported by the book \"Neural Smithing\".\n",
    "\n",
    "Convention notation to summarize structure of Multilayered Neural Network\n",
    "\n",
    "Example: 2/8/1 meaning this is a 2-layered MLP, with an input layer that has 2 nodes, and 1 hidden layer that has 8 nodes and an output layer with 1 node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f595b-a1d6-4922-991a-9489e738c71e",
   "metadata": {},
   "source": [
    "# How Artificial Neural Network work?\n",
    "1. The input is being feed as arrays of data into the input layer.\n",
    "2. Random weights are assigned to each interconnection between the input and hidden layer.\n",
    "3. The weights are multipled with the inputs and a bias is added to form the transfer function.\n",
    "$$z = \\sum_{i=1}^n w_i x_i + b$$\n",
    "\t•\t z : The result of the summation (input to the activation function).\\\n",
    "\t•\t w_i : Weight associated with each input  x_i.\\\n",
    "\t•\t x_i : Individual inputs to the node.\\\n",
    "\t•\t b : Bias term added to the summation.\\\n",
    "\t•\t $\\sum_{i=1}^n$ : Summation symbol, summing over  i  from 1 to  n  (total number of inputs).\n",
    "\n",
    "4. Weights are assigned to the interconnection between the hidden layers.\n",
    "5. The output of a transfer function (from 1 hidden layer) is fed as an input to the activation function (of the subsequent hidden layer).\n",
    "6. At the end, the output layer would output the final form of a prediction, by applying suitable activation function to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533f9d9-6221-4aab-8b3b-0994eb473350",
   "metadata": {},
   "source": [
    "# Weights\n",
    "The higher a weight of an artificial neuron is, the stronger the input which is\n",
    "multiplied by it will be. Weights can also be negative, so we can say that the signal is\n",
    "inhibited by the negative weight.\n",
    "\n",
    "Depending on the weights, the computation of the neuron will be different. By adjusting the weights of an artificial neuron we can obtain the output we want for specific inputs. \n",
    "\n",
    "But when we have an ANN of hundreds or thousands of neurons, it would be quite complicated to find by hand all the necessary weights. But we can find algorithms which can adjust the weights of the ANN in order to obtain the desired output from the network. This process of adjusting the weights is called learning or training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de713c5-0aaf-4adf-aa83-aa3084c6afa0",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "There are multiple types of activation function that could be used.\n",
    "\n",
    "1. the Sigmoid Function, $sigma(z) = \\frac{1}{1 + e^{-z}}$, which is used when the model is predicting probability.\n",
    "<img src=\"sigmoid_function_plot.png\" alt=\"Sigmoid Function\" width=\"500\">\n",
    "\n",
    "2.  the Threshold Function, $\n",
    "\\phi(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$ , which is used when the output depends on a threshold value.\n",
    "<img src=\"threshold_function_plot.png\" alt=\"Threshold Function\" width=\"500\">\n",
    "\n",
    "4. the ReLU(Rectified Linear Unit) Function, which gives an output x if x is positive, 0 otherwise.\n",
    "\n",
    "$$\\phi(x) = \\max(0, x)$$\n",
    "\n",
    "Where:\n",
    "- \\(x\\): Input value.\n",
    "- $(\\phi(x))$: Output of the ReLU function.\n",
    "<img src=\"relu_function_plot.png\" alt=\"ReLU Function\" width=\"500\">\n",
    "\n",
    "5. the Hyperbolic Tangent Function - similar to sigmoid function with a range of (-1,1).\n",
    "\n",
    "\n",
    "$$\\phi(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "Where:\n",
    "- $(x)$: Input value.\n",
    "- $(\\phi(x))$: Output of the tanh function.\n",
    "<img src=\"tanh_function_plot.png\" alt=\"Tanh Function\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f5369-43da-4e18-8345-cce832eb0066",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the process of updating the weights of the network in order to reduce the error in prediction.\\\n",
    "The backpropagation algorithm uses supervised learning, which means that we provide the algorithm with examples of the inputs and outputs we want the network to compute, and a cost function is calculated (taking into account the magnitude of loss at any point on our graph, combined with the slope).\\\n",
    "The output is compared with the original result and multiple iterations are done to get the maximum accuracy. \\\n",
    "For practical reasons, ANNs implementing the backpropagation algorithm do not\n",
    "have too many layers, since the time for training the networks grows exponentially. Also, there are refinements to the backpropagation algorithm which allow a faster learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa35daa6-9861-4cf0-ad82-eb2991f9cd4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss Function\n",
    "Loss Function is a measurement of error which defines the precision lost on comparing the predicted output to the actual output.\n",
    "\n",
    "Loss Function Formula:[(actual output) - (predicted output)]<sup>2</sup>\n",
    "\n",
    "## Error of the entire network\n",
    "The error of the network will simply be the sum of the errors of all the neurons in the output layer:\n",
    "$\\sum_{i=1}^n$[(actual output) - (predicted output)]<sup>2</sup>\n",
    "\n",
    "## Cost Function\n",
    "A function to know how far away the output of the network is from the target values.\n",
    "\n",
    "The formula for the cost function is:\n",
    "\n",
    "$$ C(w, b) = \\frac{1}{2n} \\sum_x \\left(target_x - activation_x\\right)^2 $$\n",
    "\n",
    "Where:\n",
    "- \\(n\\): Number of data points\n",
    "- \\(x\\): Represents each output neuron\n",
    "\n",
    "The activation of output neurons depends on the weights and biases of the network, so it could be thought as the cost function is a function of weights and biases of a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe28bf-5be5-4440-a631-989a288275cc",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "A graphical method of finding the minimum of a function.\\\n",
    "A random point on this curve is chosen and the slope at this point is calculated.\n",
    "- A +ve slope == an increase in weight\n",
    "- A -ve slope == a decrease in weight\n",
    "- A zero slope == appropriate weight\n",
    "  \n",
    "Our aim is to reach a point where the slope is zero.\n",
    "\n",
    "The formula for gradient descent is: $$\\Delta w_{ji} = - \\eta \\frac{\\partial E}{\\partial w_{ji}}$$\\\n",
    "\t•\t$\\Delta w_{ji}$: The change in weight for the connection from neuron $j$ to neuron $i$.\\\n",
    "\t•\t$\\eta$: The learning rate.\\\n",
    "\t•\t$\\frac{\\partial E}{\\partial w_{ji}}$: The partial derivative of the error $E$ with respect to the weight $w_{ji}$, which represents the gradient.\n",
    "\n",
    "This formula can be interpreted in the following way: the adjustment of each weight\n",
    "${\\Delta w_{ji}}$ will be the negative of a constant $\\eta$ multiplied by the dependance of the\n",
    "i previous weight on the error of the network, which is the derivative of E in respect to ${w_{i}}$.\n",
    "The size of the adjustment will depend on $\\eta$, and on the contribution of the weight to the error of the function. This is, if the weight contributes a lot to the error, the  adjustment will be greater than if it contributes in a smaller amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376071a1-b1cb-482e-b627-6605fa604332",
   "metadata": {},
   "source": [
    "# How do we know the number of layers and their types?\n",
    "Generally, you need a network large enough to capture the structure of the problem.\n",
    "Often the best network structure is found through a process of trial and error experimentation.\\\n",
    "Some research findings:\n",
    "- A multi-layered neural network with 2 hidden layers is sufficient for creating classification regions of any desired shape (Lippmann, \"An introduction to computing with neural nets\", 1987)\n",
    "- With one hidden layer that has sufficiently large amount of nodes, an MLP can approximate any function that we require (P.98, Deep Learning, 2016)\n",
    "- Yet, it is hard to know what is sufficiently large. It is more efficient to learn it with 2 (or more) hidden layers. (P.38, Neural Smithing)\n",
    "\n",
    "### 5 Approaches\n",
    "#### 1) Experimentation\n",
    "- As number of layers and number of nodes are model parameters that you need to specify during configuration, no one can tell the answer whether any configuration is efficient or not. There is a need to use a controlled experiment.\n",
    "\n",
    "#### 2) Intuition\n",
    "- The intuition can come from experience with the domain, experience with modeling problems with neural networks, or some mixture of the two.\n",
    "  \n",
    "#### 3) Go for depth\n",
    "- Deep neural networks appear to perform better (Goodfellow, Bengio, and Courville)\n",
    "  \n",
    "#### 4) Borrow Ideas\n",
    "- To leverage findings reported in literature.\n",
    "- Find research papers that describe the use of MLPs on instances of prediction problems similar in some way to your problem. Note the configuration of the networks used in those papers and use them as a starting point for the configurations to test on your problem.\n",
    "  \n",
    "#### 5)  Search\n",
    "- Design an automated search to test different network configurations.\n",
    "- Some popular search strategies include:\n",
    "    - Random: Try random configurations of layers and nodes per layer.\n",
    "    - Grid: Try a systematic search across the number of layers and nodes per layer.\n",
    "    - Heuristic: Try a directed search across configurations such as a genetic algorithm or Bayesian optimization.\n",
    "    - Exhaustive: Try all combinations of layers and the number of nodes; it might be feasible for small networks and datasets.\n",
    "\n",
    "Some ideas to reduce or manage the computational burden include:\n",
    "- Fit models on a smaller subset of the training dataset to speed up the search.\n",
    "- Aggressively bound the size of the search space.\n",
    "- Parallelize the search across multiple server instances (e.g. use Amazon EC2 service)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09166b5-8add-4d89-b784-24214de9cab4",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9a338-cce6-4e04-a05e-2499f30f2002",
   "metadata": {},
   "source": [
    "<img src=\"RBM.png\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53a18651-0ac6-43c9-b81d-25ca94f82222",
   "metadata": {},
   "source": [
    "Source: [1. Generative model that won the 2024 Physics Nobel Prize - Restricted Boltzmann Machines (RBM)](https://www.youtube.com/watch?v=Fkw0_aAtwIw&ab_channel=Serrano.Academy)\n",
    "\n",
    "A Restricted Boltzmann Machines (RBM) has 2 layers, the hidden layer on the top and the visible layer at the bottom.\\\n",
    "Each layers have some number of nodes, and there are scores with them.\\\n",
    "Every node from the visible layer is connected to every node in the hidden layer.\\\n",
    "Each edge has a weight/value.\n",
    "\n",
    "\n",
    "### RBM - Visible & Hidden Layer\n",
    "Visible layer: the one that we see in our dataset, and have to explain the behaviour of.\n",
    "Hidden layer: the one we dont see, the one that is being used to explain the behaviour of dataset.\n",
    "\n",
    "\n",
    "### RBM - Scores\n",
    "Every scenario of the nodes activated is considered.\\\n",
    "The score of a particular scenario is the summation of the scores of the nodes that is activated.\\\n",
    "All the scores of every possible scenario is being calculated.\\\n",
    "A higher score means the more likely scenario of which nodes are activated together, and a lower score or negative score means the unlikely scenario of those nodes being activated together.\n",
    "\n",
    "### Formula of RBM Energy Function\n",
    "$$E = - \\sum_i b_i v_i - \\sum_i a_i h_i - \\sum_{i,j} W_{ij} v_i h_j$$\n",
    "\n",
    "Where:\n",
    "- $E$: Energy of the system.\n",
    "- $b_i$: Bias for visible unit $v_i$.\n",
    "- $a_i$: Bias for hidden unit $h_i$.\n",
    "- $W_{ij}$: Weight between visible unit $v_i$ and hidden unit $h_j$.\n",
    "- $v_i$: Visible unit.\n",
    "- $h_i$: Hidden unit.\n",
    "\n",
    "A negative of the sum of the scores of the weights * the states of the visible layer + same sum for the hidden layer + the sum of the scores of the hidden * weights of the edges\n",
    "\n",
    "Energy = -score\n",
    "\n",
    "The joint probability distribution and partition function for the RBM are given as:\n",
    "\n",
    "$$ p(v, h) = \\frac{1}{Z} e^{-E(v, h)}$$\n",
    "\n",
    "$$Z = \\sum_{v, h} e^{-E(v, h)}$$\n",
    "\n",
    "Where:\n",
    "- $p(v, h)$: Joint probability of visible (\\(v\\)) and hidden (\\(h\\)) states.\n",
    "- $Z$: Partition function (normalizing constant).\n",
    "- $E(v, h)$: Energy associated with the visible and hidden states.\n",
    "- $\\sum_{v, h}$: Summation over all possible visible and hidden state configurations.\n",
    "\n",
    "\n",
    "### RBM - Probabilities\n",
    "Turn the scores to probabilities.\\\n",
    "First, put score to the function of $e^{score}$.\\\n",
    "Then, add all of them to get sum.\\\n",
    "Then, divide all of them by the sum to normalize them. They will add to 1.\\\n",
    "The higher the score, the higher the probability.\n",
    "\n",
    "### Training - Contrastive Divergence\n",
    "1. Go through the 1st point of dataset. Find out the possible scenarios that would be activated, and increase their probabilities. Decrease the probabilities for the rest.\n",
    "2. Continue on with the rest of points of dataset.\n",
    "In terms of formula:\n",
    "\n",
    "Find:  $$\\text{arg max}_W \\prod_{v \\in V} P(v)$$\n",
    "   - $ \\text{arg max}_W $: Find the value of $ W $ that maximizes the expression.\n",
    "   - $ \\prod_{v \\in V} P(v) $: Product over all $ v $ in $ V $.\n",
    "\n",
    "Maximize:  $$\\text{arg max}_W \\mathbb{E}[\\log P(v)]$$\n",
    "   - $ \\mathbb{E} $: Expectation operator.\n",
    "   - $ \\log P(v) $: Logarithm of the probability of $ v $.\n",
    "     \n",
    "Derivative: $$\\frac{\\partial}{\\partial W} \\log P(v_n)$$\n",
    "   - $ \\frac{\\partial}{\\partial W} $: Partial derivative with respect to $ W $.\n",
    "   - $ \\log P(v_n) $: Logarithm of the probability of $ v_n $.\n",
    "\n",
    "\n",
    "$$\\mathbb{E}\\left[\\frac{\\partial}{\\partial W} - E(v, h) \\mid v = v_n \\right] - \\mathbb{E}\\left[\\frac{\\partial}{\\partial W} - E(v, h)\\right]$$\n",
    "\n",
    "1. Expectation Operator $\\mathbb{E}$:\n",
    "   - Represents the expectation of the terms inside the brackets.\n",
    "2. Conditional Expectation $\\mid v = v_n$:\n",
    "   - Indicates the expectation is conditional on $v = v_n$.\n",
    "3. Gradient $\\frac{\\partial}{\\partial W}$:\n",
    "   - Partial derivative with respect to $W$.\n",
    "4. Energy Function $E(v, h)$:\n",
    "   - Energy of the visible $(v$) and hidden $(h$) units.\n",
    "  \n",
    "### Gibbs Sampling\n",
    "For hidden & visible layers of many nodes, the possibilities would be near endless. So there is a need for sampling.\n",
    "1. Go through the 1st point of dataset. Pick 1 of the possible scenarios that would be activated, and increase its probability.\n",
    "2. Then pick 1 of the rest at random and decrease the probabilities for that one.\n",
    "3. Proceed with the rest of points of dataset.\n",
    "\n",
    "### Updating Weights\n",
    "Increase/decrease the weights of the nodes and edges by the amount of learning rate each time.\n",
    "\n",
    "### Sampling\n",
    "If the nodes of both layers are too huge, even picking randomly from the nodes is difficult. \n",
    "#### How to pick a sample that agrees with our data point?\n",
    "Use Independent Sampling and consider the probabilities.  \n",
    "Add the weights of the relevant edges and the interested hidden node, and do a sigmoid of it to find the probability. This probability is the probabilty of whether a node of hidden layer would be activated.\n",
    "\n",
    "## Difference between RBM & a normal Feed Forward Network\n",
    "1. An RBM sends signals both \"forwards\" and \"backwards\" during inference while a feedforward network only sends signals forwards during inference.\n",
    "2. An RBM uses contrastive divergence for learning the weights and does not involve a loss function, whereas a feedforward network uses backpropagation and gradient descent for learning the weights, which requires a loss function.\n",
    "3. An RBM is energy-based (hence it has an energy function which can be said to be instead of a loss function) and follows (a simplified version of) the Boltzmann distribution (that doesn't include k and T), so it is stochastic, while a feedforward network isn't energy-based, but is instead deterministic.\n",
    "4. An RBM is an unsupervised learning method, while a feedforward network is supervised learning with targets to predict.\n",
    "5. Objective: RBM -> learns a probabilty function, ANN -> learns a complex function. \n",
    "6. What does: RBM -> estimate probable group of variables (visible and latent), ANN -> predicts output\n",
    "7. Training algorithm: ANN -> backpropagation, RBM -> contrastive divergence (Similar to 2)\n",
    "8. Basic principle: ANN -> decreases a cost function, RBM -> decreases an energy function (probability function) (Similar to 2 & 3)\n",
    "9. Weights and biases: ANN -> deterministic activation of units, RBM -> stochastic activation of units (Similar to 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc9961-d682-4398-89eb-45ab9b60a113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a38aefd-1a65-4d7d-8e77-0cef847cf7c8",
   "metadata": {},
   "source": [
    "## Introduction to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da9e95-f7e7-4e30-8cb7-dcf018a6a944",
   "metadata": {},
   "source": [
    "The inputs expected are images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55a3b6-f10d-4f22-9bf8-b7ee2dd21592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e47030-fd06-4543-9b2a-9bf6aed5548a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f13459-cd38-48fe-aa3a-765f7bec0eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a814a-b854-4d1f-bc55-6aa2647db287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf5d90-6139-4cb2-b6b2-4cb4362cde46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a445dc0f-1b22-48e8-bd3a-e5fdc08009a4",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314aa95-daaa-4c51-9977-2b2d0a5e5210",
   "metadata": {},
   "source": [
    "1. [Artificial Neural Networks for Beginners by Carlos Gershenson](https://arxiv.org/abs/cs/0308031)\n",
    "2. [Neural Network Full Course | Neural Network Tutorial For Beginners | Neural Networks | Simplilearn](https://www.youtube.com/watch?v=ob1yS9g-Zcs&ab_channel=Simplilearn)\n",
    "3. [Neural Networks in Python – A Complete Reference for Beginners](https://www.askpython.com/python/examples/neural-networks)\n",
    "4. [How to Configure the Number of Layers and Nodes in a Neural Network](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)\n",
    "5. [A Tutorial on Deep Neural Networks for Intelligent Systems](https://arxiv.org/pdf/1603.07249)\n",
    "6. [Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/absps/science.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4edfe-31e5-491f-83f3-8bcb732299a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
